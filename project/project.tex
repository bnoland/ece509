\documentclass[letterpaper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[top=1.25in, bottom=1.25in, left=1in, right=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{hyperref}

\setenumerate{parsep=0em, listparindent=\parindent}

\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\prob}{P}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}

\newtheorem{innercustomgeneric}{\customgenericname}
\providecommand{\customgenericname}{}
\newcommand{\newcustomtheorem}[2]{%
  \newenvironment{#1}[1]
  {%
   \renewcommand\customgenericname{#2}%
   \renewcommand\theinnercustomgeneric{##1}%
   \innercustomgeneric
  }
  {\endinnercustomgeneric}
}

\newcustomtheorem{customthm}{Theorem}
\newcustomtheorem{customlemma}{Lemma}

\title{Final Project}
\author{Benjamin Noland}
\date{}

\begin{document}

\maketitle

\section*{Introduction}

A common problem in applied statistics is estimation of a vector
$\beta^\ast \in \mathbb{R}^p$ of unknown but fixed parameters in the
linear model
\begin{equation} \label{eq:linear_model}
  y = X\beta^\ast + \epsilon,
\end{equation}
where $y \in \mathbb{R}^n$ is a vector of observed responses,
$X \in \mathbb{R}^{n \times p}$ is the design matrix, and
$\epsilon \in \mathbb{R}^n$ is a zero-mean random vector representing
the uncertainty in the model.

In the classical setting, we assume that the number of parameters $p$
is small relative to the number of observations, specifically
$p \leq n$. In this setting, assuming the design matrix $X$ has full
row rank, straightforward linear algebra yields an explicit, unique
least-squares estimator of $\beta^\ast$.

However, the situation when there are more parameters than
observations, i.e., $p > n$, is not so well understood, and belongs to
the active area of research known as \textit{high-dimensional
  statistics}. One of the strategies commonly employed in
high-dimensional statistics is to assume that the data is \emph{truly
  low-dimensional} in some sense. In the context of our linear model
\eqref{eq:linear_model}, this means assuming that a large number of
the entries of the true parameter vector $\beta^\ast$ are zero. To be
precise, define the \textit{support} of $\beta^\ast$ by
\begin{equation*}
  S(\beta^\ast) = \{i \in \{1, \ldots, p\} : \beta^\ast_i \neq 0\},
\end{equation*}
and let $k = |S(\beta^\ast)|$ denote its cardinality, i.e., the number
of non-zero entries of $\beta^\ast$. We assume that the vector
$\beta^\ast$ is \textit{sparse}, in the sense that $k \ll p$. Under
this \textit{sparsity assumption}, the problem reduces to that of
computing the support $S(\beta^\ast)$, allowing us to identify which
parameters in the vector $\beta^\ast$ are truly important. In this
way, we have the potential to substantially reduce the dimensionality
of the original problem.

A computational tractable method for computing estimates of the
parameters $\beta^\ast$ in the high-dimensional setting is the
\textit{LASSO} \cite{tibshirani96} (Least Absolute Shrinkage And
Selection Operator). The LASSO computes an estimate of $\beta^\ast$ as
a solution $\beta \in \mathbb{R}^p$ to the following $l_1$-constrained
quadratic program:
\begin{equation} \label{eq:constrained_problem}
  \begin{array}{ll}
    \text{minimize} & \norm{y - X\beta}_2^2 \\
    \text{subject to}
      & \norm{\beta}_1 \leq C_n
  \end{array},
\end{equation}
or equivalently, as the solution to the unconstrained problem
\begin{equation} \label{eq:unconstrained_problem}
  \text{minimize} \
    \frac{1}{2n} \norm{y - X\beta}_2^2 + \lambda_n \norm{\beta}_1,
\end{equation}
where $\lambda_n \geq 0$ is a \textit{regularization parameter} that
is in one-to-one correspondence with $C_n$ via Lagrangian duality
\cite{wainwright06}.

\section*{Project overview}

This project will explore the contributions of the paper
\cite{wainwright06} to the problem of inferring the support
$S(\beta^\ast)$ of $\beta^\ast$ (i.e., the problem of \textit{support
  recovery}) in the linear model \eqref{eq:linear_model} using the
LASSO as a means of estimating $\beta^\ast$.

\subsection*{Overview of the paper}

The paper \cite{wainwright06} provides both necessary and sufficient
conditions for the LASSO to recover the \textit{signed support}
$\mathbb{S}_\pm(\beta^\ast) \in \mathbb{R}^p$ of $\beta^\ast$ with
high probability, where $\mathbb{S}_\pm(\beta)$ is defined as follows
for any $\beta \in \mathbb{R}^p$:
\begin{equation*}
  \mathbb{S}_\pm(\beta)_i =
  \begin{cases}
    +1 & \quad \text{if $\beta_i > 0$} \\
    -1 & \quad \text{if $\beta_i < 0$} \\
    0 & \quad \text{if $\beta_i = 0$}
  \end{cases}
  \quad (i = 1, \ldots, p).
\end{equation*}
Specifically, the authors consider the following two questions:
\begin{itemize}
\item What relationships between $n$, $p$, and $k$ yield a
  \emph{unique} LASSO solution $\hat{\beta}$ satisfying
  $\mathbb{S}_\pm(\hat{\beta}) = \mathbb{S}_\pm(\beta^\ast)$?
\item For what relationships between $n$, $p$, and $k$ does \emph{no
    solution} of the LASSO yield the correct signed support?
\end{itemize}
These questions are analyzed for both deterministic designs and random
designs in the linear model \eqref{eq:linear_model}.

In addition to providing theoretical guarantees, the authors describe
the results of simulations to investigate the success/failure of the
LASSO in recovering the true signed support for random designs under
each of the following sparsity regimes:
\begin{itemize}
\item \textit{linear sparsity}: $k(p) = \ceil{\gamma p}$ for some
  $\gamma \in (0, 1)$;
\item \textit{sublinear sparsity}:
  $k(p) = \ceil{\gamma p / \log(\gamma p)}$ for some
  $\gamma \in (0, 1)$, and
\item \textit{fractional power sparsity}:
  $k(p) = \ceil{\gamma p^\delta}$ for some
  $\gamma, \delta \in (0, 1)$.
\end{itemize}
In each case, the authors take $\gamma = 0.40$ and $\delta = 0.75$,
and the number of observations $n$ is taken to be proportional to
$k\log(p - k)$. The true support of the parameter vector is chosen at
random.

For each sparsity regime and for several values of $p$, the authors
compute a sequence of values of the \textit{rescaled sample size} (or
\textit{control parameter}) $\theta = n / (k \log(p -k))$ and for each
such value, compute a sequence of corresponding LASSO solutions
$\hat{\beta}$ in order to approximate the probability
$\prob\{\mathbb{S}_\pm(\hat{\beta}) = \mathbb{S}_\pm(\beta^\ast)\}$ of
recovering the true signed support. This approximated probability is
then plotted against the control parameter $\theta$.

The first round of experiments samples the design matrix
$X \in \mathbb{R}^{n \times p}$ from a uniform Gaussian ensemble; that
is, its rows are sampled independently from the distribution
$N_p(0, I_p)$. A second round of experiments samples $X$ from a
non-uniform Gaussian ensemble; specifically, one such that the rows
are sampled independently from the distribution $N_p(0, \Sigma)$,
where $\Sigma$ is a $p \times p$ Toeplitz matrix of the form
\begin{equation*}
  \Sigma =
  \begin{pmatrix}
    1 & \mu & \mu^2 & \cdots & \mu^{p-2} & \mu^{p-1} \\
    \mu & 1 & \mu & \mu^2 & \cdots & \mu^{p-2} \\
    \mu^2 & \mu & 1 & \mu & \cdots & \mu^{p-3} \\
    \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
    \mu^{p-1} & \cdots & \mu^3 & \mu^2 & \mu & 1
  \end{pmatrix},
\end{equation*}
where $\mu = 0.10$. In both cases, the authors note good agreement
with the their theoretical predictions.

\subsection*{This project}

In addition to duplicating the simulations from the paper
\cite{wainwright06}, this project extends the simulations by
considering the more general case of \textit{elastic net} penalties
\cite{zou_hastie05}, which extend the $l_1$ penalty in
\eqref{eq:unconstrained_problem} to include an $l_2$ term as
well. Specifically, we consider solutions $\beta \in \mathbb{R}^p$ to
the problem
\begin{equation*}
  \text{minimize} \
    \frac{1}{2n} \norm{y - X\beta}_2^2
      + \lambda_n \left( \frac{1}{2} (1 - \alpha) \norm{\beta}_2^2
      + \alpha \norm{\beta}_1 \right ),
\end{equation*}
where $\alpha \in [0, 1]$ is the elastic net \textit{mixing
  parameter}. We repeat the simulations described in
\cite{wainwright06} for the case of uniform Gaussian ensembles, but
instead use elastic net solutions $\beta$ for each of $\alpha = 0.75$
and $\alpha = 0.25$ to estimate the probability of signed support
recovery. We compare the results to those from the original
simulations.

\section*{Theoretical results}

\subsection*{Unconstrained form of the problem}

As noted in the introduction, the $l_1$-constrained problem
\eqref{eq:constrained_problem} is equivalent to the unconstrained
problem \eqref{eq:unconstrained_problem} in the following sense: for
every value of $C_n$ in \eqref{eq:constrained_problem} there exists a
value $\lambda_n \geq 0$ in \eqref{eq:unconstrained_problem} such that
\eqref{eq:unconstrained_problem} is equivalent to
\eqref{eq:constrained_problem}, and vice versa (in fact, it can be
shown that $C_n$ and $\lambda_n$ are in one-to-one correspondence). We
now demonstrate this equivalence.

First, we need a lemma. We need to show that the constraint
\begin{equation} \label{eq:l1_constraint}
  \norm{\beta}_1 \leq C_n
\end{equation}
in \eqref{eq:constrained_problem} is equivalent to a finite collection
of linear equality and inequality constraints on $\beta$. Consider the
$l_1$-ball $B = \{\beta \in \mathbb{R}^p : \norm{\beta}_1 \leq
C_n\}$. Let $\{e_1, \ldots, e_p\}$ denote the standard ordered basis
for $\mathbb{R}^p$. We claim that $B = \conv S$, where
\begin{equation*}
  S = \conv\{C_n e_1, \ldots, C_n e_p, -C_n e_1, \ldots, -C_n e_p\}.
\end{equation*}
If we can show that $B = \conv S$, then $B$ is a polyhedron, so that
the constraint \eqref{eq:l1_constraint} is equivalent to a finite
collection of linear equalities and inequalities.

Note that $S \subseteq B$, and since $B$ is a convex set, and
$\conv S$ is the smallest convex set containing $S$, we have
$\conv S \subseteq B$. Conversely, let $\beta \in B$. Then there exist
$a_1, \ldots, a_p \in \mathbb{R}$ with $\beta = \sum_{i=1}^p a_i
e_i$. Assume without loss of generality that $a_1, \ldots, a_m \geq 0$
and $a_{m+1}, \ldots, a_p < 0$. Then we can write
\begin{align*}
  \beta &= \sum_{i=1}^p a_i e_i \\
        &= \sum_{i=1}^m \left ( \frac{a_i}{C_n} \right ) (C_n e_i)
          + \sum_{i=m+1}^p \left ( -\frac{a_i}{C_n} \right ) (-C_n e_i) \\
        &= \sum_{i=1}^m \left | \frac{a_i}{C_n} \right | (C_n e_i)
          + \sum_{i=m+1}^p \left | \frac{a_i}{C_n} \right | (-C_n e_i).
\end{align*}
Then the coefficients $|a_i / C_n| \geq 0$ for every
$1 \leq i \leq p$, and since $\norm{\beta}_1 \leq C_n$, we have
\begin{equation*}
  \frac{\norm{\beta}_1}{C_n}
    = \sum_{i=1}^p \left | \frac{a_i}{C_n} \right | \leq 1.
\end{equation*}
Let $K = \sum_{i=1}^p |a_i / C_n|$, so that $0 \leq K \leq 1$. Now,
$0 \in \conv S$ since we can write
$0 = (1/2) (C_n e_1) + (1/2) (-C_n e_1)$. Therefore,
\begin{equation*}
  \beta = \sum_{i=1}^m \left | \frac{a_i}{C_n} \right | (C_n e_i)
    + \sum_{i=m+1}^p \left | \frac{a_i}{C_n} \right | (-C_n e_i)
    + (1 - K) \cdot 0 \in \conv S.
\end{equation*}
This shows that $B \subseteq \conv S$, and therefore $B = \conv S$.

Now for the main argument. Let $\hat{\beta} \in \mathbb{R}^p$ be a
solution to the constrained problem
\eqref{eq:constrained_problem}. The Lagrangian of this problem is
\begin{equation*}
  L(\beta, \lambda) = \frac{1}{2n} \norm{y - X\beta}_2^2
    + \lambda(\norm{\beta}_1 - C_n),
\end{equation*}
where $\lambda \in \mathbb{R}$, and so the Lagrange dual function is
given by
\begin{equation*}
  g(\lambda) = \inf_{\beta \in \mathbb{R}^p} L(\beta, \lambda)
    = \inf_{\beta \in \mathbb{R}^p} \left [ \frac{1}{2n} \norm{y - X\beta}_2^2
      + \lambda(\norm{\beta}_1 - C_n) \right ].
\end{equation*}
Note that for any $\lambda \geq 0$, $g(\lambda) > -\infty$, so that
the dual problem
\begin{equation} \label{eq:dual_problem}
  \begin{array}{ll}
    \text{maximize} & g(\lambda) \\
    \text{subject to}
      & \lambda \geq 0
  \end{array}
\end{equation}
is always feasible. Now, since $\hat{\beta}$ is a solution to the
primal problem \eqref{eq:constrained_problem}, it in particular
satisfies $\norm{\beta}_1 \leq C_n$ (i.e., $\hat{\beta}$ is feasible
for the primal problem). By the lemma above, this $l_1$-constraint is
equivalent to a finite number of linear equality and inequality
constraints. Thus Slater's condition is satisfied for
\eqref{eq:constrained_problem}, so that strong duality holds. Since
this problem is convex and its dual problem \eqref{eq:dual_problem} is
feasible, this also implies the existence of a dual solution
$\lambda_n$. We therefore conclude that
\begin{align*}
  \hat{\beta} &= \argmin_{\beta \in \mathbb{R}^p} L(\beta, \lambda_n) \\
    &= \argmin_{\beta \in \mathbb{R}^p} \left [ \frac{1}{2n}
      \norm{y - X\beta}_2^2 + \lambda_n (\norm{\beta}_1 - C_n) \right ] \\
    &= \argmin_{\beta \in \mathbb{R}^p} \left [ \frac{1}{2n}
      \norm{y - X\beta}_2^2 + \lambda_n \norm{\beta}_1 \right ],
\end{align*}
i.e., $\hat{\beta}$ is a solution to the unconstrained problem
\eqref{eq:unconstrained_problem}.

Conversely, let $\hat{\beta} \in \mathbb{R}^p$ be a solution to the
unconstrained problem \eqref{eq:unconstrained_problem}. We claim that
$\hat{\beta}$ is a solution to the constrained problem
\eqref{eq:constrained_problem} with $C_n =
\norm{\hat{\beta}}_1$. First, note that $\hat{\beta}_1$ is clearly
feasible due to the choice of $C_n$. Suppose it is \emph{not} optimal,
i.e., there exists a feasible point $\tilde{\beta} \in \mathbb{R}^p$
with
\begin{equation*}
  \frac{1}{2n} \norm{y - X\tilde{\beta}}_2^2
    < \frac{1}{2n} \norm{y - X\hat{\beta}}_2^2.
\end{equation*}
Since $\tilde{\beta}$ is feasible,
$\norm{\tilde{\beta}}_1 \leq C_n = \norm{\hat{\beta}}_1$. Thus,
\begin{equation*}
  \frac{1}{2n} \norm{y - X\tilde{\beta}}_2^2 + \lambda_n \norm{\tilde{\beta}}_1
    < \frac{1}{2n} \norm{y - X\hat{\beta}}_2^2
      + \lambda_n \norm{\tilde{\beta}}_1
    \leq \frac{1}{2n} \norm{y - X\hat{\beta}}_2^2
      + \lambda_n \norm{\hat{\beta}}_1,
\end{equation*}
contradicting the assumption that $\hat{\beta}$ was optimal for
\eqref{eq:unconstrained_problem}. Hence $\hat{\beta}$ is a solution to
\eqref{eq:constrained_problem} with this choice of $C_n$.

\subsection*{Conditions for signed support recovery}

The paper \cite{wainwright06} provides necessary and sufficient
conditions for the LASSO to recover the signed support of the true
parameter $\beta^\ast$ in the model \eqref{eq:linear_model} for both
deterministic and random designs. Here, we restrict our attention to
the case of random design, and describe the pertinent results from
\cite{wainwright06}.

First, we need some definitions. % TODO: Do this part.

\begin{customthm}{3} \label{sufficiency}
\end{customthm}

\begin{customthm}{4} \label{necessity}
\end{customthm}

\section*{Simulations}

\subsection*{Set up}

\subsection*{Simulations from the paper}

\subsection*{Custom simulations}

\begin{thebibliography}{9}
\bibitem{wainwright06}
  Wainwright, M. (2006).
  \textit{Sharp thresholds for high-dimensional and noisy sparsity
    recovery using $l_1$-constrained quadratic programming (Lasso)}.
  Technical Report 709, Dept. Statistics, Univ. California,
  Berkeley

\bibitem{tibshirani96}
  Tibshirani, R. (1996).
  \textit{Regression shrinkage and selection via the Lasso}.
  J. Roy. Statist. Soc. Ser. B \textbf{58} 267--288

\bibitem{zou_hastie05}
  Zou, H. and Hastie, T. (2005)
  \textit{Regularization and variable selection via the elastic net}
  J. Roy. Statist. Soc. Ser. B \textbf{67} 301--320

\end{thebibliography}

\end{document}

\documentclass[letterpaper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[top=1.25in, bottom=1.25in, left=1in, right=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{enumitem}

\setenumerate{parsep=0em, listparindent=\parindent}

\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\spn}{span}

\title{Project Proposal}
\author{Benjamin Noland}
\date{}

\begin{document}

\maketitle

\section*{Problem description}

A common problem in applied statistics is estimation of a vector
$\beta^\ast \in \mathbb{R}^p$ of unknown but fixed parameters in the
linear model
\begin{equation} \label{eq:linear_model}
  y = X\beta^\ast + \epsilon,
\end{equation}
where $y \in \mathbb{R}^n$ is a vector of observed responses,
$X \in \mathbb{R}^{n \times p}$ is the design matrix, and
$\epsilon \in \mathbb{R}^n$ is a zero-mean random vector representing
the uncertainty in the model.

In the classical setting, we assume that the number of parameters $p$
is small relative to the number of observations, specifically
$p \leq n$. In this setting, assuming the design matrix $X$ has full
row rank, straightforward linear algebra yields an explicit, unique
least-squares estimator of $\beta^\ast$.

However, the situation when there are more parameters than
observations, i.e., $p > n$, is not so well understood, and belongs to
the active area of research known as \textit{high-dimensional
  statistics}. One of the strategies commonly employed in
high-dimensional statistics is to assume that the data is \emph{truly
  low-dimensional} in some sense. In the context of our linear model
\eqref{eq:linear_model}, this means assuming that a large number of
the entries of the true parameter vector $\beta^\ast$ are zero. To be
precise, define the \textit{support} of $\beta^\ast$ by
\begin{equation*}
  S(\beta^\ast) = \{i \in \{1, \ldots, p\} : \beta^\ast_i \neq 0\},
\end{equation*}
and let $k = |S(\beta^\ast)|$ denote its cardinality, i.e., the number
of non-zero entries of $\beta^\ast$. We assume that the vector
$\beta^\ast$ is \textit{sparse}, in the sense that $k \ll p$. Under
this \textit{sparsity assumption}, the problem reduces to that of
computing the support $S(\beta^\ast)$, allowing us to identify which
parameters in the vector $\beta^\ast$ are truly important. In this
way, we have the potential to substantially reduce the dimensionality
of the original problem.

A computational tractable method for computing estimates of the
parameters $\beta^\ast$ in \eqref{eq:linear_model} is the LASSO

%\section*{Simulations}

\begin{thebibliography}{9}
\bibitem{wainwright06}
  Wainwright, M. (2006).
  \textit{Sharp thresholds for high-dimensional and noisy sparsity
    recovery using $l_1$-constrained quadratic programming (Lasso)}.
  Technical Report 709, Dept. Statistics, Univ. of California,
  Berkeley

\bibitem{fan01}
  Fan, J. and Li, R. (2001).
  \textit{Variable selection via nonconcave penalized likelihood and
    its oracle properties}.
  Journal of the American Statistical Association, 96:456, 1348-1360,
  DOI: 10.1198/016214501753382273

\end{thebibliography}

\end{document}
